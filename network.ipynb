{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"network.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"eNUt_-E7NuX3","colab_type":"text"},"source":["# Training model"]},{"cell_type":"markdown","metadata":{"id":"Xxb9AycWN3lZ","colab_type":"text"},"source":["In this notebook:\n","\n","1.   Data preparation\n","2.   Download Xception model with ImageNet weights\n","3.   Add top layers for binary classification task on top of Xception\n","4.   Short train with Xception layers frozen\n","5.   Long train all layers until best validation loss value found, using early stopping\n","\n"]},{"cell_type":"code","metadata":{"id":"k_WQynGnBlU-","colab_type":"code","outputId":"55814b9a-14e9-41b6-d94b-8168c4ce81d2","executionInfo":{"status":"ok","timestamp":1586945945820,"user_tz":-120,"elapsed":359,"user":{"displayName":"Michał Bernacki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnmPN0hwphGAGXExHQyanC5d6OYykk08Li2O2BHg=s64","userId":"02963925927313730455"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-5Ut-fMb9J_L","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","import IPython.display as display\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import os\n","import pathlib"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HvEEBg4WJfI2","colab_type":"code","outputId":"cb3baca8-f9b6-4db4-c8b7-720d005fe1b4","executionInfo":{"status":"ok","timestamp":1586945974013,"user_tz":-120,"elapsed":745,"user":{"displayName":"Michał Bernacki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnmPN0hwphGAGXExHQyanC5d6OYykk08Li2O2BHg=s64","userId":"02963925927313730455"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["#Paths to the dataset\n","train_dir = '/content/gdrive/My Drive/dataset/1_train'\n","train_dir = pathlib.Path(train_dir)\n","\n","valid_dir = '/content/gdrive/My Drive/dataset/2_validation'\n","valid_dir = pathlib.Path(valid_dir)\n","\n","#Inspect number of photos in each subset\n","train_count = len(list(train_dir.glob('*/*.png')))\n","valid_count = len(list(valid_dir.glob('*/*.png')))\n","print(\"Number of training photos:\", train_count, \"\\nNumber of validation photos:\", valid_count)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of training photos: 5600 \n","Number of validation photos: 800 \n","Number of test photos: 800\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6o5TKfbSSzJO","colab_type":"code","outputId":"cf874cc3-8601-4646-b978-631596754439","executionInfo":{"status":"ok","timestamp":1586945981406,"user_tz":-120,"elapsed":581,"user":{"displayName":"Michał Bernacki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnmPN0hwphGAGXExHQyanC5d6OYykk08Li2O2BHg=s64","userId":"02963925927313730455"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["CLASS_NAMES = np.array(['bad', 'good'])\n","CLASS_NAMES[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'bad'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"zg3l7MNuzFyL","colab_type":"code","colab":{}},"source":["#This batch size provides same size for every batch\n","#Image dimensions required for the input to Xception network\n","BATCH_SIZE = 32\n","IMG_HEIGHT = 229\n","IMG_WIDTH = 229\n","\n","#Steps for training and validation purposes\n","STEPS_PER_EPOCH = np.ceil(train_count/BATCH_SIZE)\n","STEPS_PER_EPOCH_V = np.ceil(valid_count/BATCH_SIZE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FsbnvMxv9J_0","colab_type":"code","outputId":"ec096173-b218-48bb-9830-48b35a80d112","executionInfo":{"status":"ok","timestamp":1586341165776,"user_tz":-120,"elapsed":2641,"user":{"displayName":"Michał Bernacki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnmPN0hwphGAGXExHQyanC5d6OYykk08Li2O2BHg=s64","userId":"02963925927313730455"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#Train batches generator\n","train_datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=keras.applications.xception.preprocess_input)\n","\n","train_generator = train_datagen.flow_from_directory(directory=str(train_dir),\n","                                                    batch_size=BATCH_SIZE,\n","                                                    shuffle=True,\n","                                                    target_size=(IMG_HEIGHT, IMG_WIDTH),\n","                                                    class_mode='binary')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 5600 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UL_iKIE4KccJ","colab_type":"code","outputId":"90ffb996-a8a6-48e6-ad21-96c719507e99","executionInfo":{"status":"ok","timestamp":1586341165778,"user_tz":-120,"elapsed":2626,"user":{"displayName":"Michał Bernacki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnmPN0hwphGAGXExHQyanC5d6OYykk08Li2O2BHg=s64","userId":"02963925927313730455"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#Validation batches generator\n","valid_datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=keras.applications.xception.preprocess_input)\n","\n","valid_generator = valid_datagen.flow_from_directory(directory=str(valid_dir),\n","                                                    batch_size=BATCH_SIZE,\n","                                                    shuffle=True,\n","                                                    target_size=(IMG_HEIGHT, IMG_WIDTH),\n","                                                    class_mode='binary')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 800 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"frDoHnS59KAE","colab_type":"code","outputId":"a9cd7367-49b8-4c3f-8cf9-5259aea12c9a","executionInfo":{"status":"ok","timestamp":1586341216959,"user_tz":-120,"elapsed":10342,"user":{"displayName":"Michał Bernacki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnmPN0hwphGAGXExHQyanC5d6OYykk08Li2O2BHg=s64","userId":"02963925927313730455"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#Import Xception model with weights trained on ImageNet dataset, without last layer\n","base_model = keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\n","avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n","output = keras.layers.Dense(1, activation=\"sigmoid\")(avg)\n","model = keras.models.Model(inputs=base_model.input, outputs=output)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n","83689472/83683744 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IjO0w3WR9KAJ","colab_type":"code","outputId":"21358ecc-2692-4b55-ef2d-bc4796bbd4db","executionInfo":{"status":"ok","timestamp":1586344124060,"user_tz":-120,"elapsed":2537656,"user":{"displayName":"Michał Bernacki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnmPN0hwphGAGXExHQyanC5d6OYykk08Li2O2BHg=s64","userId":"02963925927313730455"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["#Freezing all Xception layers for first 5 epochs\n","#Unfreezed remain only avg and output layer\n","for layer in base_model.layers:\n","    layer.trainable = False\n","\n","checkpoint_cb = keras.callbacks.ModelCheckpoint(\"/content/gdrive/My Drive/image_x_model_fr.h5\", save_best_only=True)\n","optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n","model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"binary_accuracy\"])\n","history = model.fit(train_generator,\n","                    steps_per_epoch=STEPS_PER_EPOCH,\n","                    validation_data=valid_generator,\n","                    validation_steps=STEPS_PER_EPOCH_V,\n","                    epochs=5,\n","                    callbacks=[checkpoint_cb])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","175/175 [==============================] - 2324s 13s/step - loss: 0.9044 - binary_accuracy: 0.7989 - val_loss: 0.5126 - val_binary_accuracy: 0.8075\n","Epoch 2/5\n","175/175 [==============================] - 48s 274ms/step - loss: 0.5981 - binary_accuracy: 0.8264 - val_loss: 0.4536 - val_binary_accuracy: 0.8363\n","Epoch 3/5\n","175/175 [==============================] - 47s 270ms/step - loss: 0.3990 - binary_accuracy: 0.8539 - val_loss: 0.4739 - val_binary_accuracy: 0.8300\n","Epoch 4/5\n","175/175 [==============================] - 48s 274ms/step - loss: 0.3685 - binary_accuracy: 0.8618 - val_loss: 0.4448 - val_binary_accuracy: 0.8225\n","Epoch 5/5\n","175/175 [==============================] - 48s 276ms/step - loss: 0.3360 - binary_accuracy: 0.8673 - val_loss: 0.4161 - val_binary_accuracy: 0.8475\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i242fXTG9KAN","colab_type":"code","outputId":"12a3963f-3c9b-4b0a-baa7-af36a795e17d","executionInfo":{"status":"ok","timestamp":1586345693114,"user_tz":-120,"elapsed":1391100,"user":{"displayName":"Michał Bernacki","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgnmPN0hwphGAGXExHQyanC5d6OYykk08Li2O2BHg=s64","userId":"02963925927313730455"}},"colab":{"base_uri":"https://localhost:8080/","height":663}},"source":["#Unfreezing base model layers for next epochs\n","for layer in base_model.layers:\n","    layer.trainable = True\n","\n","checkpoint_cb = keras.callbacks.ModelCheckpoint(\"/content/gdrive/My Drive/image_x_model_fr.h5\", save_best_only=True)\n","early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n","optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True, decay=0.001)\n","model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"binary_accuracy\"])\n","history = model.fit(train_generator,\n","                    steps_per_epoch=STEPS_PER_EPOCH,\n","                    validation_data=valid_generator,\n","                    validation_steps=STEPS_PER_EPOCH_V,\n","                    epochs=100,\n","                    callbacks=[checkpoint_cb, early_stopping_cb])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","175/175 [==============================] - 74s 424ms/step - loss: 0.2561 - binary_accuracy: 0.8832 - val_loss: 0.1201 - val_binary_accuracy: 0.9550\n","Epoch 2/100\n","175/175 [==============================] - 72s 409ms/step - loss: 0.1160 - binary_accuracy: 0.9450 - val_loss: 0.1292 - val_binary_accuracy: 0.9475\n","Epoch 3/100\n","175/175 [==============================] - 74s 425ms/step - loss: 0.0665 - binary_accuracy: 0.9677 - val_loss: 0.0909 - val_binary_accuracy: 0.9600\n","Epoch 4/100\n","175/175 [==============================] - 74s 424ms/step - loss: 0.0511 - binary_accuracy: 0.9786 - val_loss: 0.0869 - val_binary_accuracy: 0.9638\n","Epoch 5/100\n","175/175 [==============================] - 72s 411ms/step - loss: 0.0437 - binary_accuracy: 0.9802 - val_loss: 0.1004 - val_binary_accuracy: 0.9638\n","Epoch 6/100\n","175/175 [==============================] - 74s 423ms/step - loss: 0.0327 - binary_accuracy: 0.9854 - val_loss: 0.0714 - val_binary_accuracy: 0.9750\n","Epoch 7/100\n","175/175 [==============================] - 72s 410ms/step - loss: 0.0268 - binary_accuracy: 0.9871 - val_loss: 0.0767 - val_binary_accuracy: 0.9700\n","Epoch 8/100\n","175/175 [==============================] - 72s 411ms/step - loss: 0.0232 - binary_accuracy: 0.9882 - val_loss: 0.0798 - val_binary_accuracy: 0.9700\n","Epoch 9/100\n","175/175 [==============================] - 74s 423ms/step - loss: 0.0241 - binary_accuracy: 0.9880 - val_loss: 0.0584 - val_binary_accuracy: 0.9750\n","Epoch 10/100\n","175/175 [==============================] - 72s 411ms/step - loss: 0.0171 - binary_accuracy: 0.9927 - val_loss: 0.0895 - val_binary_accuracy: 0.9725\n","Epoch 11/100\n","175/175 [==============================] - 72s 411ms/step - loss: 0.0160 - binary_accuracy: 0.9920 - val_loss: 0.0738 - val_binary_accuracy: 0.9737\n","Epoch 12/100\n","175/175 [==============================] - 72s 411ms/step - loss: 0.0163 - binary_accuracy: 0.9923 - val_loss: 0.0694 - val_binary_accuracy: 0.9812\n","Epoch 13/100\n","175/175 [==============================] - 72s 411ms/step - loss: 0.0154 - binary_accuracy: 0.9916 - val_loss: 0.0848 - val_binary_accuracy: 0.9700\n","Epoch 14/100\n","175/175 [==============================] - 72s 411ms/step - loss: 0.0134 - binary_accuracy: 0.9937 - val_loss: 0.0709 - val_binary_accuracy: 0.9737\n","Epoch 15/100\n","175/175 [==============================] - 72s 411ms/step - loss: 0.0128 - binary_accuracy: 0.9943 - val_loss: 0.0751 - val_binary_accuracy: 0.9787\n","Epoch 16/100\n","175/175 [==============================] - 72s 412ms/step - loss: 0.0107 - binary_accuracy: 0.9955 - val_loss: 0.0998 - val_binary_accuracy: 0.9688\n","Epoch 17/100\n","175/175 [==============================] - 72s 411ms/step - loss: 0.0137 - binary_accuracy: 0.9932 - val_loss: 0.0939 - val_binary_accuracy: 0.9700\n","Epoch 18/100\n","175/175 [==============================] - 72s 411ms/step - loss: 0.0126 - binary_accuracy: 0.9929 - val_loss: 0.0853 - val_binary_accuracy: 0.9737\n","Epoch 19/100\n","175/175 [==============================] - 72s 412ms/step - loss: 0.0129 - binary_accuracy: 0.9939 - val_loss: 0.0646 - val_binary_accuracy: 0.9787\n"],"name":"stdout"}]}]}